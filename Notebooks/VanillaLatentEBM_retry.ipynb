{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.13.1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPUtil'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2254380/293364249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mGPUtil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshowUtilization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgpu_usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPUtil'"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "# Set Seaborn style\n",
    "sns.set(style='darkgrid', font_scale=1.2)\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "print(\"Using torch\", torch.__version__)\n",
    "#torch.manual_seed(42) # Setting the seed\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.profiler\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class langevin_sampler():\n",
    "    def __init__(self, sampling_steps, sampling_noise, device):\n",
    "        self.device = device\n",
    "        self.K = torch.tensor(sampling_steps, device=self.device)\n",
    "        self.s = torch.tensor(sampling_noise, device=self.device)\n",
    "    \n",
    "    def get_sample(self, initial_sample, model, x):\n",
    "        x_k = initial_sample\n",
    "        z_k = x\n",
    "        step=0\n",
    "        \n",
    "        while step < self.K:\n",
    "            # Compute gradient\n",
    "            grad = model.grad_log_fn(x_k, z_k)\n",
    "            \n",
    "            # Update sample\n",
    "            x_k = x_k + self.s * grad + (torch.sqrt(2*self.s)*torch.randn_like(x_k, device=self.device))  \n",
    "            \n",
    "            step += 1             \n",
    "        \n",
    "        return x_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class priorEBM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "                    nn.Linear(input_dim, 256),  # Increase the number of units in the first layer\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),  # Adjust the number of units based on the complexity\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, output_dim),\n",
    "                    nn.Tanh()  # Use an appropriate activation function based on the desired range\n",
    "                )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # Returns f_a(z)\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def grad_log_fn(self, z, x):\n",
    "        f_z = self.forward(z)\n",
    "        grad_f_z = torch.autograd.grad(outputs=f_z, inputs=z, grad_outputs=torch.ones_like(f_z), create_graph=True)[0]\n",
    "        return grad_f_z - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class topdownGenerator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, sample_steps, sample_noise, SIGMA):\n",
    "        super().__init__()\n",
    "        self.K = sample_steps\n",
    "        self.s = sample_noise\n",
    "        self.sigma = SIGMA\n",
    "        self.layers = nn.Sequential(\n",
    "                    nn.Linear(input_dim, 256), \n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, output_dim),\n",
    "                    nn.Tanh()  \n",
    "                )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        g_z = self.layers(z)\n",
    "        return g_z + self.sigma*torch.randn_like(g_z)\n",
    "    \n",
    "    def grad_log_fn(self, z, x):\n",
    "        g_z = self.forward(z)\n",
    "        grad_g_z = torch.autograd.grad(outputs=g_z, inputs=z, grad_outputs=torch.ones_like(g_z), create_graph=True)[0]\n",
    "        return grad_g_z * (x - g_z) / (self.sigma**2) - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(x, x_pred, sigma):\n",
    "    neg_log_likelihood = 0.5 * torch.square(x - x_pred) / sigma**2\n",
    "    return torch.mean(neg_log_likelihood)\n",
    "\n",
    "def EBM_loss(z_prior, z_posterior, EBMmodel):\n",
    "            en_neg = torch.mean(EBMmodel(z_prior.detach()), dim=1)\n",
    "            en_pos = torch.mean(EBMmodel(z_posterior.detach()), dim=1)\n",
    "\n",
    "            return torch.mean(en_neg - en_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pr478@ad.eng.cam.ac.uk/repos/IIB-Project-ScoreNets/Notebooks/VanillaLatentEBM_retry.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pr478%40ad.eng.cam.ac.uk/repos/IIB-Project-ScoreNets/Notebooks/VanillaLatentEBM_retry.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m SAMPLE_NOISE \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pr478%40ad.eng.cam.ac.uk/repos/IIB-Project-ScoreNets/Notebooks/VanillaLatentEBM_retry.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m NUM_Z_SAMPLES \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pr478%40ad.eng.cam.ac.uk/repos/IIB-Project-ScoreNets/Notebooks/VanillaLatentEBM_retry.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pr478%40ad.eng.cam.ac.uk/repos/IIB-Project-ScoreNets/Notebooks/VanillaLatentEBM_retry.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Load the moons dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pr478%40ad.eng.cam.ac.uk/repos/IIB-Project-ScoreNets/Notebooks/VanillaLatentEBM_retry.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m X, _ \u001b[39m=\u001b[39m make_moons(n_samples\u001b[39m=\u001b[39mSAMPLES, noise\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3000\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "SAMPLE_STEPS = 20\n",
    "SAMPLES = 5000\n",
    "SIGMA = 1\n",
    "SAMPLE_NOISE = 1\n",
    "NUM_Z_SAMPLES = 100\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the moons dataset\n",
    "X, _ = make_moons(n_samples=SAMPLES, noise=0.05, random_state=42)\n",
    "X = torch.tensor(X).float().to(device)\n",
    "loader = DataLoader(X, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create the model and optimizer\n",
    "EBMmodel = priorEBM(X.shape[1], X.shape[1]).to(device)\n",
    "GENmodel= topdownGenerator(X.shape[1], X.shape[1], SAMPLE_STEPS, SAMPLE_NOISE, SIGMA).to(device)\n",
    "Sampler = langevin_sampler(SAMPLE_STEPS, SAMPLE_NOISE, device)\n",
    "\n",
    "EBMoptimiser = torch.optim.Adam(EBMmodel.parameters(), lr=LR)\n",
    "EBMlossfn = EBM_loss\n",
    "\n",
    "GENoptimiser = torch.optim.Adam(GENmodel.parameters(), lr=LR)\n",
    "GENlossfn = generator_loss\n",
    "\n",
    "# Write to tensorboard 10 times\n",
    "sample_every = NUM_EPOCHS//10\n",
    "writer = SummaryWriter(f\"runs/VanillaEBM\")\n",
    "num_plots = (NUM_EPOCHS // sample_every) - 1\n",
    "num_cols = min(5, num_plots)  # Maximum of 2 columns\n",
    "num_rows = (num_plots - 1) // num_cols + 1\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "fig.suptitle(\"Generated Samples\")\n",
    "\n",
    "tqdm_bar = tqdm(range(NUM_EPOCHS))\n",
    "\n",
    "with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./runs/VanillaEBM/profilerlogs'),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True\n",
    ") as prof:\n",
    "    for epoch in tqdm_bar:\n",
    "        total_loss = 0\n",
    "        for batch_idx, x in enumerate(loader): \n",
    "            batch_size = x.shape[0]\n",
    "            \n",
    "            # Repeat each x sample NUM_Z_SAMPLES times\n",
    "            x = x.repeat(NUM_Z_SAMPLES, 1)\n",
    "\n",
    "            # 1a. Sample from latent prior\n",
    "            z0_noise = torch.randn_like(x, device=device, requires_grad=True)\n",
    "\n",
    "            # 1b. Expontentially tilt z, sample from posterior\n",
    "            z_prior = Sampler.get_sample(z0_noise, EBMmodel, None) # p(z|M)\n",
    "            z_posterior = Sampler.get_sample(z_prior, GENmodel, x) # p(z|x, M)\n",
    "            # 2. Forward pass on generation\n",
    "            x_pred = GENmodel(z_posterior)\n",
    "\n",
    "            # Reshape x_pred and x to have shape (BATCH_SIZE, NUM_Z_SAMPLES, -1)\n",
    "            x_pred = x_pred.reshape(batch_size, NUM_Z_SAMPLES, -1)\n",
    "            x = x.reshape(batch_size, NUM_Z_SAMPLES, -1)\n",
    "\n",
    "            # 3. Compute generation loss\n",
    "            loss_g = generator_loss(x, x_pred, SIGMA)\n",
    "\n",
    "            # 4. Backpropagation\n",
    "            GENoptimiser.zero_grad()\n",
    "            loss_g.backward()\n",
    "\n",
    "            # 5. Update weights\n",
    "            GENoptimiser.step()\n",
    "\n",
    "            # 6. Compute EBM loss\n",
    "            EBMoptimiser.zero_grad()\n",
    "            loss_e = EBMlossfn(z_prior, z_posterior, EBMmodel)\n",
    "            loss_e.backward()\n",
    "\n",
    "            # 7. Update weights\n",
    "            EBMoptimiser.step()\n",
    "\n",
    "            total_loss += loss_g.item() + loss_e.item()\n",
    "            \n",
    "            prof.step()\n",
    "            if batch_idx >= 1 + 1 + 3:\n",
    "                break\n",
    "\n",
    "        tqdm_bar.set_description(f\"Epoch {epoch}: Loss: {total_loss / (BATCH_SIZE):.4f}\")\n",
    "\n",
    "        if epoch % sample_every == 0 or epoch == NUM_EPOCHS:\n",
    "            \n",
    "            # Need gradients for langevin sampling\n",
    "            z0_noise = torch.randn_like(x, device=device, requires_grad=True)\n",
    "            z_prior = Sampler.get_sample(z0_noise, EBMmodel, None) # p(z|M)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = GENmodel(z_prior)\n",
    "                samples = x.detach().cpu()\n",
    "\n",
    "                plot_num = (epoch // sample_every) - 1\n",
    "                row = plot_num // num_cols\n",
    "                col = (plot_num % num_cols)\n",
    "                axs[row, col].clear()\n",
    "                axs[row, col].scatter(x=samples[:, 0].numpy(), y=samples[:, 1].numpy(), color='red', marker='o')\n",
    "                axs[row, col].set_title(f'Epoch: {epoch}')\n",
    "\n",
    "                # Convert the Matplotlib figure to a NumPy array\n",
    "                fig.canvas.draw()\n",
    "                image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "                image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "                # Write the image to TensorBoard\n",
    "                writer.add_image(\"VanillaEBM -- Make Moons\", image, global_step=epoch, dataformats='HWC')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatentEBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
