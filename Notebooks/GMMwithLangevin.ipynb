{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 2.1.0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import numpy as np \n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "# Set Seaborn style\n",
    "sns.set(style='darkgrid', font_scale=1.2)\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "print(\"Using torch\", torch.__version__)\n",
    "#torch.manual_seed(42) # Setting the seed\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.profiler\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class langevin_sampler():\n",
    "    def __init__(self, sampling_steps, sampling_noise, device):\n",
    "        self.device = device\n",
    "        self.K = torch.tensor(sampling_steps, device=self.device)\n",
    "        self.s = torch.tensor(sampling_noise, device=self.device)\n",
    "    \n",
    "    def get_sample(self, initial_sample, model, x):\n",
    "        x_k = initial_sample\n",
    "        z_k = x\n",
    "        step=0\n",
    "        \n",
    "        while step < self.K:\n",
    "            # Compute gradient\n",
    "            grad = model.grad_log_fn(x_k, z_k)\n",
    "            \n",
    "            # Update sample\n",
    "            x_k = x_k + self.s * grad + (torch.sqrt(2*self.s)*torch.randn_like(x_k, device=self.device))  \n",
    "            \n",
    "            step += 1             \n",
    "        \n",
    "        return x_k\n",
    "\n",
    "class GaussianMixtureModel(nn.Module):\n",
    "    def __init__(self, num_components, dim, device):\n",
    "        super(GaussianMixtureModel, self).__init__()\n",
    "        self.num_components = num_components\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(num_components, device=self.device) / num_components)\n",
    "        self.means = nn.Parameter(torch.randn(num_components, dim, device=self.device))\n",
    "        self.variances = nn.Parameter(torch.ones(num_components, dim, device=self.device))\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        log_probs = []\n",
    "        for i in range(self.num_components):\n",
    "            log_prob = -0.5 * torch.sum(torch.log(2 * torch.tensor(3.1416, device=self.device) * self.variances[i]) +\n",
    "                                       (x - self.means[i]) ** 2 / self.variances[i], dim=1)\n",
    "            log_probs.append(torch.log(self.weights[i]) + log_prob)\n",
    "        log_probs = torch.stack(log_probs, dim=1)\n",
    "        return torch.logsumexp(log_probs, dim=1)\n",
    "\n",
    "    def grad_log_fn(self, x, z):\n",
    "        log_probs = []\n",
    "        for i in range(self.num_components):\n",
    "            log_prob = -0.5 * torch.sum(torch.log(2 * torch.tensor(3.1416, device=self.device) * self.variances[i]) +\n",
    "                                       (x - self.means[i]) ** 2 / self.variances[i], dim=1)\n",
    "            log_probs.append(torch.log(self.weights[i]) + log_prob)\n",
    "        log_probs = torch.stack(log_probs, dim=1)\n",
    "\n",
    "        probs = torch.exp(log_probs - torch.logsumexp(log_probs, dim=1, keepdim=True))\n",
    "\n",
    "        grad_weights = probs\n",
    "        grad_means = probs.unsqueeze(-1) * (x.unsqueeze(1) - self.means)\n",
    "        grad_variances = probs.unsqueeze(-1) * (((x.unsqueeze(1) - self.means) ** 2 / self.variances) - 1)\n",
    "\n",
    "        return torch.cat((torch.sum(grad_weights, dim=0), torch.sum(grad_means, dim=0), torch.sum(grad_variances, dim=0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 1 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Sample from the GMM using the Langevin sampler\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mget_sample(torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice), gmm, X)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(sample, X)  \u001b[38;5;66;03m# Define your loss function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mlangevin_sampler.get_sample\u001b[0;34m(self, initial_sample, model, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Compute gradient\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     grad \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgrad_log_fn(x_k, z_k)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Update sample\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     x_k \u001b[38;5;241m=\u001b[39m x_k \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m+\u001b[39m (torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn_like(x_k, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))  \n",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m, in \u001b[0;36mGaussianMixtureModel.grad_log_fn\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m     54\u001b[0m grad_means \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans)\n\u001b[1;32m     55\u001b[0m grad_variances \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (((x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariances) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39msum(grad_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39msum(grad_means, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), torch\u001b[38;5;241m.\u001b[39msum(grad_variances, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 1 and 2"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "NUM_EPOCHS = 500\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-4\n",
    "SAMPLE_STEPS = 20\n",
    "SAMPLES = 500\n",
    "SAMPLE_NOISE = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate 'make_moons' data\n",
    "X, _ = make_moons(n_samples=SAMPLES, noise=0.05, random_state=42)\n",
    "X = torch.tensor(X).float().to(device)\n",
    "\n",
    "# Create the Gaussian Mixture Model\n",
    "gmm = GaussianMixtureModel(num_components=2, dim=2, device=device)\n",
    "sampler = langevin_sampler(SAMPLE_STEPS, SAMPLE_NOISE, device)\n",
    "loss_fn = nn.MSELoss()  # Define your loss function\n",
    "optimizer = torch.optim.Adam(gmm.parameters(), lr=LR)  # Define your optimizer\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Sample from the GMM using the Langevin sampler\n",
    "    sample = sampler.get_sample(torch.randn(1, 2, device=device), gmm, X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(sample, X)  # Define your loss function\n",
    "\n",
    "    # Perform backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # Printing progress\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch [{epoch}/{NUM_EPOCHS}], Loss: ...\")  # Update loss calculation here\n",
    "\n",
    "# Plot the generated samples\n",
    "plt.scatter(X[:, 0].cpu(), X[:, 1].cpu(), color='blue', label='Original Data')\n",
    "plt.scatter(sample[:, 0].cpu(), sample[:, 1].cpu(), color='red', label='Generated Samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LatentEBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
